---
title: "Alzheimer's Analysis"
author: "Sneha Jacob, Alina Bangash, Jeremy Udo, Sana Akhtar, Jake Schwartz"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
=======
# Introduction 
Our team decided to analyze the Alzheimer’s Disease dataset created by Rabei El Kharoua on Kaggle. We decided to proceed with this topic since Alzheimer’s is a very common illness without a cure that affects millions of people both directly and indirectly. This data set includes data from 2,149 patients and has 34 variables. Through this analysis we wanted to answer the question: Which factors are strongly associated with the diagnosis of Alzheimer's Disease? 

## Dataset Information:
- Age: age of patients (60-90 years)\
- Gender: 0 if male or 1 if female\
- Ethnicity: 0 - Caucasian, 1 - African American, 2 - Asian, 3 - other\
- Education Level: 0 - None, 1 - High School, 2 - Bachelors, 3 - Higher\
- BMI: Body Mass Index (15 - 40)\
- Smoking: 0 - No, 1 - Yes\
- Physical Activity: Weekly physical activity (0 - 10 hrs)\
- DietQuality: Diet quality score (0 -10)\
- SleepQuality: Sleep quality score (4 - 10)\
- Health History Variables (Categorical: 0 - No, 1 - Yes): 
    - FamilyHistoryAlzheimers
    - CardiovascularDisease
    - Diabetes
    - Depression 
    - HeadInjury
    - Hypertension\
- MMSE: Mini-Mental State Examination score (0 -30); lower score indicates cognitive impairment\
- FunctionalAsssessment: (0 -10); lower score indicates greater impairment\
- MemoryComplaints: 0 - No, 1- Yes\
- BehavioralProblems: 0 - No, 1 - Yes\
- ADL: Activities of Daily Living score, (0 -10); lower score indicates greater impairment\
- Symptom Variables (Categorical: 0 - No, 1 - Yes): 
    - Confusion
    - Disorientation
    - PersonalityChanges 
    - DifficultyCompletingTasks
    - Forgetfulness\
- Diagnosis: Diagnosis Status for Alzheimer’s Disease; 0 - No, 1 - Yes

# Neural Network (Jake Schwartz, Sana Akhtar)

First, let's examine our neural network model, which predicts whether a person has Alzheimer's based on various input variables. The model starts with 32 input features, which are processed through two hidden layers before reaching the output layer. Each connection between nodes has an associated weight, representing the importance of the input variables in making predictions as they pass through the hidden layers. Additionally, each node includes a bias term, a constant value that helps adjust the output along with the weighted input, improving the model's accuracy and flexibility. 
  
  ![Neural Network Diagram](NeuralNet.png)

Input layer: n = 32

Hidden layer 1: n = 64

Hidden layer 2: n = 32

Output layer: n = 1

## Model Information
  
The model employs the ReLU activation function, which enhances the efficiency of training and the overall performance of neural networks, particularly when dealing with non-linear data. ReLU introduces non-linearity by allowing positive values to pass through unchanged while setting negative values to zero. This not only accelerates the convergence of the training process but also helps mitigate the vanishing gradient problem, a common issue in deep networks. The ReLU improves the network's ability to learn complex patterns and relationships within the data.

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("ReLu.png")
```

The neural network model uses Binary Cross Entropy with Logits (BCE with Logits) to measure the loss between the predicted probabilities and the true labels. This function first applies a sigmoid activation to the raw, unnormalized outputs (logits) to convert them into probabilities. By combining the sigmoid function and BCE in a single step, BCE with Logits provides greater numerical stability compared to applying a sigmoid function followed by a separate BCE loss calculation. This approach ensures more reliable training, especially in neural networks.

![BCE w/Logits Equation](BCE_LogitsEQ.png)

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("sigmoid.png")
```

## Model in Code

Using the PyTorch library in R, we created our neural network model with the nn_module, defining the specifics of the layers and functions. The complete neural network architecture designed to predict Alzheimer's in patients includes an input layer with 32 nodes, a first hidden layer with 64 nodes, a second hidden layer with 32 nodes, and an output layer with a single node that produces a binary output (Yes or No). This structure allows the model to effectively learn from the input features and make accurate predictions.

```{r eval=FALSE}
library(torch)

alzheimers_net = nn_module(
  "class_net",
  
  initialize = function(){
    self$layer1 = nn_linear(in_features = ncol(x), out_features = 64)
    self$layer2 = nn_linear(in_features = 64, out_features = 32)
    self$output = nn_linear(in_features = 32, out_features = 1)
  },
  forward = function(x){
    x %>%
      self$layer1() %>%
      nnf_relu() %>%
      self$layer2() %>%
      nnf_relu() %>%
      self$output()
  }
)
```

We evaluated the neural network model using a 10-fold cross-validation procedure, with each iteration consisting of 200 epochs. During the cross-validation, as the number of epochs approached 200, the loss consistently decreased, indicating effective learning. Utilizing the ReLU activation function and the BCE with Logits loss function, we achieved a final loss of 0.90%. This comprehensive evaluation resulted in a cross-validation classification accuracy of 98.37% and an error rate of 1.63%, demonstrating the model's strong predictive performance for identifying Alzheimer's in patients.

```{r eval=FALSE}
# Sample code loop: 
library(caret)
k <- 10
folds <- createFolds(y, k = k, list = TRUE, returnTrain = TRUE)
all_losses <- list()
result <- numeric(k)
for (i in 1:k) {
  cat("Fold:", i, "\n")
  # Split the data
  # Initialize the model
  # Training loop
  num_epochs <- 200
  for (epoch in 1:num_epochs) {
    #Step through network
  }
  # Evaluate the model
  neural_model$eval()
  with_no_grad({
    outputs <- neural_model(x_test)
    predictions <- torch_sigmoid(outputs) > 0.5
    accuracy <- mean(as_array(predictions) == as_array(y_test))
    result[i] <- accuracy
  })
}
```

## Conclusion

Next, we visualized the loss for each training fold and epoch. We observed that around the 200th epoch, the loss began to plateau, indicating that additional training did not significantly reduce the loss further. Overall, the model demonstrated strong accuracy in predicting whether a person has Alzheimer's, showcasing its effectiveness and reliability for this classification task.

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("Epoch_training_loss.png")
```

# Decision Tree (Jeremy Udo, Alina Bangash, Sneha Jacob):

Our second model will be using a decision tree to evaluate which variables are strongly associated with Alzheimer’s disease. We selected this model because our response variable is qualitative, and decision trees are well-suited for this type of data. By utilizing a decision tree, we can easily interpret data and be able to deal with non-linear relationships. Decision trees have proven to handle categorical data effectively and can easily visualize the probability of developing Alzheimer's disease by using all of the predictors. Additionally, with this model, we are able to prune the tree in order to improve the model and the error rate. We initially started with a decision tree model and then applied pruning to enhance its performance. We then compared the results of both models to evaluate the effectiveness of decision trees in predicting the likelihood of developing Alzheimer's disease.

## Model Formula: 
Our model formula for the decision tree is: \
*Diagnosis ~ PatientID + Age + Gender + Ethnicity + ... + DificultyCompletingTasks + Forgetfulness + Diagnosis + DoctorInCharge*

We decided to go forth with a decision tree since our response variable is qualitative. For the unpruned tree, we decided to use all thirty-five variables. Using this tree-based model allowed for an easier understanding of which factors are associated with the diagnosis of Alzheimer's Disease and through the classification tree, we can get a better understanding of which factors are most important through pruning. \
```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("unprunedtree.png")
```
```{r}
library(tree)

build_decision_tree <- function(train_data) {
  decision_tree <- tree(Diagnosis ~ ., data = train_data)
  return(decision_tree)
}

```

For this base tree we had preprocessed the data to remove any rows with missing values and then split the dataset into eighty percent for training and the remaining twenty for testing. Using the training set we visualized and evaluated the decision tree and noticed a 93.95% accuracy rate and an error rate of 6.04%, demonstrating the unpruned tree's ability to accurately predict whether or not a person is diagnosed with Alzheimer's Disease. 

# Conclusion: 

















